---
title: "Reading Obama Search on NY Times via Lexis Nexis"
author: "Fred Boehm"
date: "March 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 
 
I downloaded the 500 most recent articles that appeared when searching NY Times on Lexis Nexis for the term "Obama". 

I downloaded the resulting txt file directly from Lexis Nexis and saved it in the "data" directory.

We now read that file into R. 


```{r read-nyt}
tx <- scan(file = "~/bret-research/twitter/aejmc-manuscript-2016/data/The_New_York_Times2016-03-17_16-01.TXT", what = "character", blank.lines.skip = TRUE, sep = "\n", encoding = "UTF-8", skipNul = TRUE) 
```

We now partition the object `tx` into separate files.

```{r}
library(wordtools) #install my R package 'wordtools'
tx_list <- split_tx(tx = tx, patt = "Copyright 20")
```

We examined the first 3 articles. It looks like the 8th line is always "LENGTH: ___ words". Let's verify this:

```{r}
library(stringr)
sapply(FUN = function(x)which(str_detect(string = x, pattern = "LENGTH")), X = tx_list) 
# We were wrong!!
tx_list2<- sapply(FUN = function(x)x[-(1:which(str_detect(string = x, pattern = "LENGTH")))], X = tx_list) 
```

We now need to remove text lines at the end of each article.

```{r remove-trailing, eval = FALSE}
tx_list3 <- sapply(FUN = function(x)x[1:(-1 + which(str_detect(string = x, pattern = "URL")))], X = tx_list2)
```

We saw that the code above gives an error, since some articles don't have a URL line.

Let's remove the blog posts then see if remaining articles have URL field.

```{r chekc}
myfun <- function(x, pattern = "Web Blog"){
  collapsed <- paste(x, collapse = " ")
  !stringr::str_detect(collapsed, pattern = pattern)
}
  

indswb <- sapply(FUN = myfun, X = tx_list2)
indsurl <- sapply(FUN = myfun, pattern = "URL", X = tx_list2)
cbind(indswb, indsurl)
```

It looks like only the blog posts have no URL field. Let's remove the blog posts and then pipe it to remove the lines that contain URL and any later lines.

```{r}
library(magrittr)
good_art <- tx_list2[indswb] %>% 
  sapply(FUN = function(x){x[-(which(str_detect(x, "URL")):length(x))]})
```

Now we need to separate strings into individual words & remove punctuation.

```{r split_and_remove_punctuation}
library(tm)
stopwords <- tm::stopwords("SMART")
good2 <- sapply(FUN = function(x)paste(x, collapse = " "), X = good_art)  %>%
  stringr::str_split( pattern = " ") %>%
  sapply(FUN = function(x) gsub("'", "", x)) %>% # remove apostrophes
  sapply(FUN = function(x) gsub("[[:punct:]]", " ", x)) %>%  # replace punctuation with space
  sapply(FUN = function(x) gsub("[[:cntrl:]]", " ", x)) %>% # replace control characters with space
  sapply(FUN = function(x) gsub("^[[:space:]]+", "", x)) %>% # remove whitespace at beginning of documents
  sapply(FUN = function(x) gsub("[[:space:]]+$", "", x)) %>% # remove whitesp
  sapply(FUN = function(x)tolower(x)) %>%
  sapply(FUN = function(x)x[!(x == "")]) %>% # remove elements that are ""
  sapply(FUN = function(x)x[!(x %in% stopwords)]) # remove stopwords
```




```{r}
# from http://cpsievert.github.io/LDAvis/reviews/reviews.html
# compute the table of terms:
n_min <- 3
term_table <- table(unlist(good2)) %>% 
  sort(decreasing = TRUE)
term_table <- term_table[term_table >= n_min]
vocab <- names(term_table)
get_terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(good2, get_terms)
```

```{r, summary_stats}
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term_table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
```

```{r lda, cache = TRUE}
# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 12 minutes on laptop
```

```{r wordcloud}
library(wordcloud)
for (i in 1:K){
  cloud.data<-sort(fit$topics[i,], decreasing=TRUE)[1:50]
  wordcloud(names(cloud.data), freq=cloud.data, scale=c(3,.10), min.freq=1, rot.per=0, random.order=FALSE, col = 1+ i %% 4)
}
```






# Here, we use $K = 10$


```{r lda-10, cache = TRUE}
# MCMC and model tuning parameters:
K <- 10
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 12 minutes on laptop
```


```{r wordcloud-10}
library(wordcloud)
for (i in 1:K){
  cloud.data<-sort(fit$topics[i,], decreasing=TRUE)[1:50]
  wordcloud(names(cloud.data), freq=cloud.data, scale=c(3,.10), min.freq=1, rot.per=0, random.order=FALSE, col = 1+ i %% 4)
}
```



# Here, we use $K = 30$



```{r lda-30, cache = TRUE}
# MCMC and model tuning parameters:
K <- 30
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 12 minutes on laptop
```

```{r wordcloud-30}
library(wordcloud)
for (i in 1:K){
  cloud.data<-sort(fit$topics[i,], decreasing=TRUE)[1:50]
  wordcloud(names(cloud.data), freq=cloud.data, scale=c(3,.10), min.freq=1, rot.per=0, random.order=FALSE, col = 1+ i %% 4)
}
```



