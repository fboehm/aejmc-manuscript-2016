---
title: Data Analysis with Topic Models for Communications Researchers
output: 
  pdf_document:
    toc: false
    number_sections: true
    includes:
      in_header: header.tex
fontsize: 12pt
geometry: margin=1in
bibliography: twitter.bib
csl: apa.csl
---

# Abstract

We present a non-technical introduction to topic modeling for communications researchers. We discuss important statistical models and illustrate these methods with example data from communications scholarship. We complement our discussion with computer code (in the R computing language) that implements these methods. We close with thoughts about the future value of topic modeling to communications researchers.



# Motivation

We are in the big data era. Social media inundates us with status updates and tweets, while bloggers share their views on current events. Website creation has become easier than ever. These new media interact with more established media, such as print news media, television, and radio. What do they have in common? They all can be viewed as data sources in which words - whether written or spoken, tweeted or blogged - are the data. 

Big data presents big opportunities for communications researchers. We present a concise summary of a collection of machine learning techniques that, together, are called "topic modeling". We discuss how these methods may be used in communications research, and we apply topic models to illustrative examples to demonstrate their value to communications research questions.

# Overview

With the tremendous rise in computing speed and memory capacity over the last quarter century, researchers working at the interface of quantitative methods and social sciences gained the capacity to treat written texts as data. While document analysis is still in its infancy, scientists nevertheless have made great progress towards computational dissection and interpretation of texts. Among the most foundational contributions is the development of probabilistic topic models. We detail below, with limited use of statistical terminology, how these methods work and why they may be useful in communications research. We also provide computer code (in the R programming language) that implements these methods.




# Latent Dirichlet Allocation

@blei2003latent introduced a (generative) statistical model called "latent dirichlet allocation" (LDA) in 2003. Although others had described similar statistical models [@pritchard2000inference], @blei2003latent first applied the statistical model to text analysis. Researchers in a wide range of disciplines - including genetics, linguistics, psychology, political science, and others - have enthusiastically adopted LDA and related methods. 



## What is the intuition behind the model?

As @blei2012probabilistic writes, the key to understanding LDA is to recognize that a given document - be it a research article, a novel, or a blog post - exhibits multiple topics. Each topic, in turn, is, in a technical sense, a distribution over words. For example, a topic related to evolution may heavily weight the words "evolution", "evolutionary", "biology", "phylogenetic", and "species". In a given collection of documents, which we term a "corpus" of documents, we assume that relatively few topics - on the order of 10 to 50 for most analyses - are present. 

## In what sense is the model 'generative'?

We say that LDA is a generative model because we specify a joint probability distribution that allows generation of observed samples. In a more precise manner, we specify a distribution over topics (again, where topics are themselves distributions over words) that is shared by the corpus. Each document can be viewed as a draw from this distribution. The observed topic (distribution over words) tells us how the words are distributed. 


# LDA with statistical terminology

We can also describe LDA with more formal statistical terminology. We let $\beta_1, \beta_2, ..., \beta_K$ be the $K$ topics, where each $\beta_k$ is a distribution over the vocabulary. Topic weights (or proportions) we denote by $\theta_d$ for the $d^{th}$ document. We let $\theta_{d,k}$ be the topic proportion for topic $k$ in document $d$. Topic assignments for the $d^{th}$ document are $z_d$, where $z_{d,n}$ is the topic assignment for the $n^{th}$ word in the $d^{th}$ document. Observed words for the $d^{th}$ document are $w_d$, where $w_{d, n}$ is the $n^{th}$ word for the $d^{th}$ document.

We then use the above notation to specify the joint distribution for all variables in the model:

$$p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) = \prod_{i = 1}^Kp(\beta_i)\prod_{d = 1}^Dp(\theta_d)\left( \prod_{n = 1}^N p(z_{d,n}|\theta_d)p(w_{d,n}|\beta_{1:K}, z_{d,n})\right)$$

The vertical bars in the last expressions denote conditional probabilities. Note that the topic assignments $z_{d,n}$ depend on the document's topic proportions. Whatismore, the observed word $w_{d,n}$ has a distribution that depends on both the entire collection of topics, $\beta_{1:K}$, and the topic, $z_{d,n}$, assigned to that word.

# Example of LDA with print media

An example may help to illustrate our point. 

- which example to use here?

We analyzed 


@blei2007correlated fitted a 100-topic model to 17,000 research articles from the journal Science. They found that a 1996 article "Seeking Life's Bare (Genetic) Necessities" [@pennisi1996seeking] exhibited topics related to "evolution", "genetics", "disease", and "computers".
**Would it be more sensible to use RT's Super Bowl results here?**






## What is the model?

LDA models have a hiearchical structure in which words make up documents, and a collection of documents is a corpus. The corpus is assumed to have (unobserved) topics, or themes. The purposes of LDA, then, are to discover the unobserved topics from the texts and to characterize documents by the topics that they contain.

We distinguish generative models from discriminative models. A generative model, such as LDA, is one in which we specify a joint probability distribution for all variables (including those that are unobserved) and, thus, enable the creation of observations. On the other hand, a discriminative model relies on a conditional distribution and doesn't permit generation of samples from the joint distribution. 





## What are its assumptions?





## What are its limitations? 

The original LDA model However, LDA's flexibility and adaptability have reduced the impact of its initial limitations. In the last decade, researchers have devised extensions of LDA, such as "dynamic LDA", that enable one to model topic evolution over time. Another extension of LDA, which is known as "correlated LDA", accounts for correlations among topics. 


# Inference in topic models

Existing strategies for fitting topic models can be divided into two classes: 1. variational Bayes methods and 2. sampling methods. Variational Bayes methods try to approximate the posterior distribution by maximizing a lower bound. In this sense, variational Bayes methods approach topic model fitting from a computer science optimization perspective. Alternatively, sampling methods, such as those based on Markov chain monte carlo (MCMC) approaches, draw samples from (an approximation to) the posterior distribution and estimate distributional parameters by their empirical sample statistics. 

Computational implementations of both classes of strategies are freely available for the R statistical computing language. 

# Interpreting results of topic modeling

Statistical inference for topic models yields estimates for topics (i.e., distributions over words), assignments of words (within documents) to topics, and weights of topics in each document. 

# Visualizing topics

- LDAvis
- wordcloud

Visualizing topic modeling results is an active area of research. We present below strategies that involve static and interactive displays. 

  Word clouds are a widely used method for presenting topic modeling results. Unfortunately, the standard approach requires a distinct word cloud for each topic. For models with more than ten topics, manually examining wordclouds becomes unwieldy. 
  For a single topic, the default method for creating a word cloud involves the identification of the most heavily weighted words in the topic. The user may choose to assign font sizes that are proportional to the weights of each word.
  One newly developed method, which is implemented in the LDAvis R package, uses a singular value decomposition of the fitted document-topic matrix to calculate principal components. Each topic is then plotted in two dimensions, where the axes represent the first two principal components. The LDAvis R package enhances this second approach by making the figures interactive with D3 javascript.
  
  
  



# Results from Tweets Analysis



# Discussion



# Future directions



# Online resources

David Mimno, a Cornell University scholar, curates an annotated bibliography of topic modeling research[@mimno2016topic]. His bibliography is available at this url: http://mimno.infosci.cornell.edu/topics.html


# Computational implementation of LDA with R

We present below instructions and code for using LDA in the R statistical computing language [@r2015].
**Ask Dhavan about including code?** **Maybe as an appendix?**




# References





