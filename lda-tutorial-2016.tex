\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{unicode=true,
            pdftitle={Data Analysis with Topic Models for Communications Researchers},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Data Analysis with Topic Models for Communications Researchers}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{}
  \preauthor{}\postauthor{}
  \date{}
  \predate{}\postdate{}


\usepackage{setspace}
\doublespacing
\usepackage[nomarkers,figuresonly]{endfloat}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{ifthen}

% \usepackage[top=10mm, bottom=10mm, left=10mm, right=10mm,includehead,includefoot]{geometry}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}

% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\begin{document}
\maketitle

\pagestyle{fancy}

\listoftodos

\section{Abstract}\label{abstract}

We present a non-technical introduction to data analysis with topic
models for communications researchers. We motivate the discussion with a
research question from social media communications research. We then
discuss statistical aspects of topic models as we illustrate these
methods with data from Twitter. We complement our discussion with
computer code (in the R computing language) that implements these
methods. We close with thoughts about the future value of topic modeling
to communications researchers.

\section{Motivation}\label{motivation}

\todo[color=pink]{Can we write a scenario like that of Blei's 2012 article?}

We are in the big data era. Social media inundates us with status
updates and tweets, while bloggers share their views on current events.
These new media interact with more established media, such as print news
media, television, and radio. What do they have in common? One answer is
that they all can be viewed as data sources in which words - whether
written or spoken, tweeted or blogged - are the data.

Think about how we currently browse media. Let's suppose that we want to
read about mass shootings. We might search for the key words ``mass''
and ``shooting''. Doing so, we will find some articles that deal with
mass shootings, but other results may deal with other types of
``shootings'', such as film shooting or accidental shooting. Even if we
specify that the two-word phrase ``mass shooting'' must appear in our
results, we still may find some articles that merely mention mass
shootings without focusing on one or more mass shootings.

Blei (2012) envisions an alternative search strategy in which we search
for articles that are \emph{about} ``mass shootings''. Some of the
resulting articles would have the phrase ``mass shooting'', but
containing that two-word phrase is not required; the sole requirement is
that the article focus on one or more mass shootings. That is, we'd like
to see all of the articles that have the theme, or ``topic'', mass
shooting. Topic modeling is one method that may ultimately enable us to
do the theme-based browsing that Blei (2012) suggests.

``Topic modeling'' refers to a collection of statistical and machine
learning methods that have the goal of discovering latent topics in a
large collection, or ``corpus'', of texts. The tremendous rise in
computing speed and memory capacity, coupled with the increasing
availability of digitized texts, has enabled researchers working at the
interface of quantitative methods and social sciences to treat written
texts as data. While data analysis of documents is still in its infancy,
scientists nevertheless have made great progress towards computational
dissection and interpretation of texts. We detail below, with limited
use of statistical terminology, how these methods work and why they may
be useful in communications research. We illustrate topic modeling
methods in the analysis of all New York Times articles from three days
in March 2016. We also provide an appendix with computer code (in the R
programming language) that implements these methods.

\section{Background}\label{background}

LDA is an extension of an earlier text analysis method called
``probabilistic latent semantic analysis'' (pLSA) (Hofmann, 1999).
Hofmann (1999) described pLSA as a ``latent class model for factor
analysis of count data''. The novelty of LDA, compared to pLSA, is that
LDA places a Dirichlet process prior on the probability distributions of
words. This use of a Dirichlet process prior makes computations
convenient because of a Bayesian statistical concept called
``conjugacy''. That is, the posterior distribution, which is the object
of statistical inference, is also a Dirichlet process.

In the 13 years since the publication of Blei, Ng, \& Jordan (2003),
researchers have developed a wide variety of extensions to LDA. Many of
these extensions of LDA accommodate relationships among topics. Widely
used extensions of LDA include correlated topic models (Blei \&
Lafferty, 2007), hierarchical topic models (D. Griffiths \& Tenenbaum,
2004), author-topic models (Rosen-Zvi, Griffiths, Steyvers, \& Smyth,
2004), and dynamic topic models (Blei \& Lafferty, 2006). Looking
towards the future, researchers will need to fine tune and extend
existing methods to accommodate new text data structures. One current
research frontier is in topic modeling of tweets on Twitter. Twitter
data present challenges because they are streaming and each message is
no more than 140 characters.

Topic models have found applications in a wide range of disciplines.
Pritchard, Stephens, \& Donnelly (2000) used a statistical model much
like LDA with genetic marker data to infer population structure and
relatedness in human subjects. DeRose, Roy, \& Boehm (2014) and Roy,
DeRose, \& Boehm (2013) used LDA (and other methods) in efforts to
characterize themes from a collection of Victorian novels. Other
examples include a 2008 study by Hall, Jurafsky, \& Manning (2008), in
which they applied LDA to examine the history of ideas. Grimmer (2010)
applied a hierarchical topic model to study agendas in Senate press
releases.

\section{Box's Loop \& Data analysis}\label{boxs-loop-data-analysis}

\begin{figure}
\includegraphics{box-blei-loop.png}
\caption{Iterative process for building, computing, critiquing, and applying topic models.\label{fig:box}}
\end{figure}

The perspective that guides our data analysis is based on ideas of the
University of Wisconsin-Madison statistician George Box and coworkers.
Blei (2014) uses the ideas of Box and colleagues in the specific case of
iterative refinement of topic models. Box
(CITATION)\todo[color=purple]{What works - from Box \& colleagues - are cited by Blei 2014??}
articulated a process of scientific inquiry and statistical
collaboration in which scientists put forth a hypothesis about the
natural world, and, with assistance and guidance from statisticians,
design experiments to test the scientific hypothesis. After statistical
analysis of the experimental data, the scientific hypothesis is refined,
which leads to design and implementation of another experiment, and the
iterative process between scientific hypothesis and experiment
continues. Box also suggested (CITATION) that a statistician, working in
collaboration with scientists, might use scientific questions as
motivation to develop new statistical methods in experimental design and
data analysis. In this sense, there is a second iterative loop by which
a statistical researcher develops novel methods because of their
immediate need to answer a scientific research question. Blei (2014)
adapts these iterative processes to the case of latent variable models,
such as LDA and related models. He argues that we view the use of a
topic model for a specific data analysis task as an iterative procedure
in which one proposes a simple topic model for the data analysis, fits
the model, interprets the results, and then, if needed, refines the
topic model with the goal of achieving data analysis results that are
more consistent with the research goals.

\section{Latent Dirichlet Allocation}\label{latent-dirichlet-allocation}

Blei et al. (2003) introduced a (generative) statistical model called
``latent dirichlet allocation'' (LDA) in 2003. Although others had
described similar statistical models (Pritchard et al., 2000), Blei et
al. (2003) first applied the statistical model to text analysis.
Researchers in a wide range of disciplines - including genetics,
linguistics, psychology, political science, and others - have
enthusiastically adopted LDA and related methods.

\subsection{What is the intuition behind the
model?}\label{what-is-the-intuition-behind-the-model}

As Blei (2012) writes, the key to understanding LDA is to recognize that
a given document - be it a research article, a novel, or a blog post -
exhibits multiple topics. Each topic, in turn, is, in a technical sense,
a probability distribution over words. For example, a topic related to
evolution may heavily weight the words ``evolution'', ``evolutionary'',
``biology'', ``phylogenetic'', and ``species''. In a given collection of
documents, which we term a ``corpus'' of documents, we assume that
relatively few topics - on the order of 10 to 50 for most analyses - are
present.

\subsection{\texorpdfstring{In what sense is the model
`generative'?}{In what sense is the model generative?}}\label{in-what-sense-is-the-model-generative}

We say that LDA is a generative model because we specify a joint
probability distribution that allows generation of observed samples. In
a more precise manner, we specify a probability distribution over topics
(again, where topics are themselves probability distributions over
words) that is shared by the corpus. Each document can be viewed as a
draw from this probability distribution. The observed topic (probability
distribution over words) tells us how the words are distributed.

\subsection{What is the model?}\label{what-is-the-model}

LDA models have a hiearchical structure in which words make up
documents, and a collection of documents is a corpus. The corpus is
assumed to have (unobserved) topics, or themes. The purposes of LDA,
then, are to discover the unobserved topics from the texts and to
characterize documents by the topics that they contain.

We distinguish generative models from discriminative models. A
generative model, such as LDA, is one in which we specify a joint
probability distribution for all variables (including those that are
unobserved) and, thus, enable the creation of observations. On the other
hand, a discriminative model relies on a conditional probability
distribution and doesn't permit generation of samples from the joint
probability distribution.

\subsection{What are its assumptions?}\label{what-are-its-assumptions}

\subsection{What are its limitations?}\label{what-are-its-limitations}

The original LDA model However, LDA's flexibility and adaptability have
reduced the impact of its initial limitations. In the last decade,
researchers have devised extensions of LDA, such as ``dynamic LDA'',
that enable one to model topic evolution over time. Another extension of
LDA, which is known as ``correlated LDA'', accounts for correlations
among topics.

\section{LDA with statistical
terminology}\label{lda-with-statistical-terminology}

We can also describe LDA with more formal statistical terminology. We
let \(\beta_1, \beta_2, ..., \beta_K\) be the \(K\) topics, where each
\(\beta_k\) is a probability distribution over the vocabulary. Topic
weights (or proportions) we denote by \(\theta_d\) for the \(d^{th}\)
document. We let \(\theta_{d,k}\) be the topic proportion for topic
\(k\) in document \(d\). Topic assignments for the \(d^{th}\) document
are \(z_d\), where \(z_{d,n}\) is the topic assignment for the
\(n^{th}\) word in the \(d^{th}\) document. Observed words for the
\(d^{th}\) document are \(w_d\), where \(w_{d, n}\) is the \(n^{th}\)
word for the \(d^{th}\) document.

We then use the above notation to specify the joint probability
distribution for all variables in the model:

\[p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) = \prod_{i = 1}^Kp(\beta_i)\prod_{d = 1}^Dp(\theta_d)\left( \prod_{n = 1}^N p(z_{d,n}|\theta_d)p(w_{d,n}|\beta_{1:K}, z_{d,n})\right)\]

The vertical bars in the last expressions denote conditional
probabilities. Note that the topic assignments \(z_{d,n}\) depend on the
document's topic proportions. Whatismore, the observed word \(w_{d,n}\)
has a probability distribution that depends on both the entire
collection of topics, \(\beta_{1:K}\), and the topic, \(z_{d,n}\),
assigned to that word.

\section{Example of LDA with print
media}\label{example-of-lda-with-print-media}

An example may help to illustrate our approach. Let's suppose that we
want to understand the themes, or topics, that appear in a collection of
print media articles. One strategy is to read every article in the
collection. We could then assign one or more themes to each article and,
at the end of our analysis, we would have a collection of themes. An
alternative approach is to use topic modeling to identify topics, which
we treat as themes, in the collection of articles.

To demonstrate our topic modeling approach to discovering themes, or
topics, in print media, we used the Lexis Nexis Academic database to
acquire 746 New York Times (print) articles from March 19, 20, and 21 of
2016.

We read the raw downloaded text files into R, then partitioned the files
into the 746 articles. To each article, we applied a series of text
processing steps. We split each article into its distinct words and
removed white space and punctuation before converting all text to
lower-case. At this stage, we had a collection of words for each of the
746 distinct articles. After removing words that occurred fewer than
three times in the collection of articles and those that appeared on our
``stop list'' of frequently occurring short words, including articles
and prepositions, we created the vocabulary and the document-term
matrix. We then used existing R code (from the ``lda'' R package) to
perform collapsed Gibbs sampling to sample from the posterior
probability distribution of the parameters. Note that this computational
implementation requires us to specify the number of topics in the
collection. We arbitrarily chose 20 topics for our initial topic models.

\section{Inference in topic models}\label{inference-in-topic-models}

Existing strategies for fitting topic models can be divided into two
classes: 1. variational Bayes methods and 2. sampling methods.
Variational Bayes methods try to approximate the posterior probability
distribution by maximizing a lower bound. In this sense, variational
Bayes methods approach topic model fitting from a computer science
optimization perspective. Alternatively, sampling methods, such as those
based on Markov chain monte carlo (MCMC) approaches, draw samples from
(an approximation to) the posterior probability distribution and
estimate distributional parameters by their empirical sample statistics.

Computational implementations of both classes of strategies are freely
available for the R statistical computing language. We include code to
use these free implementations in the appendix.

\section{Interpreting results of topic
modeling}\label{interpreting-results-of-topic-modeling}

Statistical inference for topic models yields estimates for topics
(i.e., probability distributions over words), assignments of words
(within documents) to topics, and weights of topics in each document.

The user must impose meaning on the topics. For instance, a topic may
put heavy weights on the words ``genetics'', ``gene'', ``regulation'',
``DNA'', ``transcription'', and ``RNA''. The data analyst needs to
identify the similarities among these words - namely, that they describe
concepts related to genetics and molecular biology. Fortunately, topic
modeling procedures often generate topics that can be summarized in one
or two words once the data analyst has inspected the most heavily
weighted terms (for a given topic).

\section{Visualizing topics}\label{visualizing-topics}

Visualizing topic modeling results is an active area of research. We
present below strategies that involve static and interactive displays.
Word clouds are a widely used method for presenting topic modeling
results. Unfortunately, the standard approach requires a distinct word
cloud for each topic. For models with more than ten topics, manually
examining wordclouds becomes unwieldy. Word cloud construction is
straightforward and implemented with freely available software, such as
the R package ``wordcloud''. For a single topic, the default method for
creating a word cloud involves the identification of the most heavily
weighted words in the topic. The user may choose to assign font sizes
that are proportional to the weights of each word. One newly developed
method, which is implemented in the LDAvis R package, uses a singular
value decomposition of the fitted document-topic matrix to calculate
principal components. Each topic is then plotted in two dimensions,
where the axes represent the first two principal components. The LDAvis
R package enhances this second approach by making the figures
interactive with D3 javascript. For our analysis of 746 New York Times
articles, in which our initial topic models featured 20 topics, we
created wordclouds for each topic in the resulting models.

\section{Results from NY Times Articles
Analysis}\label{results-from-ny-times-articles-analysis}

\begin{figure}
\includegraphics[width=\textwidth]{lda-tutorial-2016_files/figure-latex/wordcloud1-1.pdf}
\caption{Wordcloud for one topic from a 20-topic model of 746 New York Times articles. Larger font size corresponds to greater weight of that word in this topic.\label{fig:wc1}}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{lda-tutorial-2016_files/figure-latex/wordcloud1-2.pdf}
\caption{Wordcloud for one topic from a 20-topic model of 746 New York Times articles. Larger font size corresponds to greater weight of that word in this topic.\label{fig:wc2}}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{lda-tutorial-2016_files/figure-latex/wordcloud1-3.pdf}
\caption{Wordcloud for one topic from a 20-topic model of 746 New York Times articles. Larger font size corresponds to greater weight of that word in this topic.\label{fig:wc3}}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{lda-tutorial-2016_files/figure-latex/wordcloud1-4.pdf}
\caption{Wordcloud for one topic from a 20-topic model of 746 New York Times articles. Larger font size corresponds to greater weight of that word in this topic.\label{fig:wc4}}
\end{figure}

\section{Discussion}\label{discussion}

We see that the

\section{Future directions}\label{future-directions}

\section{Online resources}\label{online-resources}

We've posted our code, results, and manuscript on github.com. To access
these materials, please visit this url:
\url{http://github.com/fboehm/aejmc-manuscript-2016}

David Mimno, a Cornell University scholar, curates an annotated
bibliography of topic modeling research(Mimno, 2016). His bibliography
is available at this url:
\url{http://mimno.infosci.cornell.edu/topics.html}

\section{Computational implementation of LDA with
R}\label{computational-implementation-of-lda-with-r}

We present below instructions and code for using LDA (with a collapsed
Gibbs sampler) in the R statistical computing language (R Core Team,
2015). \textbf{Ask Dhavan about including code?} \textbf{Maybe as an
appendix?}

Now we show all the code chunks:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(knitr)}
\NormalTok{opts_chunk$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{echo =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{message =} \OtherTok{FALSE}\NormalTok{)  }\CommentTok{# hide source code in the document}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\NormalTok{devtools::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"fboehm/wordtools"}\NormalTok{)}
\NormalTok{tx1 <-}\StringTok{ }\KeywordTok{scan}\NormalTok{(}\DataTypeTok{file =} \StringTok{"~/bret-research/twitter/aejmc-manuscript-2016/data/The_New_York_Times2016-03-21_17-02.TXT"}\NormalTok{, }
    \DataTypeTok{what =} \StringTok{"character"}\NormalTok{, }\DataTypeTok{blank.lines.skip =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{encoding =} \StringTok{"UTF-8"}\NormalTok{, }
    \DataTypeTok{skipNul =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{tx2 <-}\StringTok{ }\KeywordTok{scan}\NormalTok{(}\DataTypeTok{file =} \StringTok{"~/bret-research/twitter/aejmc-manuscript-2016/data/The_New_York_Times2016-03-21_17-04.TXT"}\NormalTok{, }
    \DataTypeTok{what =} \StringTok{"character"}\NormalTok{, }\DataTypeTok{blank.lines.skip =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{encoding =} \StringTok{"UTF-8"}\NormalTok{, }
    \DataTypeTok{skipNul =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{tx <-}\StringTok{ }\KeywordTok{c}\NormalTok{(tx1, tx2)}
\KeywordTok{library}\NormalTok{(wordtools)  }\CommentTok{#install my R package 'wordtools'}
\NormalTok{tx_list <-}\StringTok{ }\KeywordTok{split_tx}\NormalTok{(}\DataTypeTok{tx =} \NormalTok{tx, }\DataTypeTok{patt =} \StringTok{"Copyright 20"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(stringr)}
\NormalTok{tx_list2 <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) x[-(}\DecValTok{1}\NormalTok{:}\KeywordTok{which}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(}\DataTypeTok{string =} \NormalTok{x, }\DataTypeTok{pattern =} \StringTok{"LENGTH"}\NormalTok{)))], }
    \DataTypeTok{X =} \NormalTok{tx_list)}
\NormalTok{myfun <-}\StringTok{ }\NormalTok{function(x, }\DataTypeTok{pattern =} \StringTok{"Web Blog"}\NormalTok{) \{}
    \NormalTok{collapsed <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(x, }\DataTypeTok{collapse =} \StringTok{" "}\NormalTok{)}
    \NormalTok{!stringr::}\KeywordTok{str_detect}\NormalTok{(collapsed, }\DataTypeTok{pattern =} \NormalTok{pattern)}
\NormalTok{\}}


\NormalTok{indswb <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{myfun, }\DataTypeTok{X =} \NormalTok{tx_list2)}
\NormalTok{indsurl <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{myfun, }\DataTypeTok{pattern =} \StringTok{"URL"}\NormalTok{, }\DataTypeTok{X =} \NormalTok{tx_list2)}
\KeywordTok{library}\NormalTok{(magrittr)}
\NormalTok{good_art <-}\StringTok{ }\NormalTok{tx_list2[indswb] %>%}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) \{}
    \NormalTok{x[-(}\KeywordTok{which}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(x, }\StringTok{"URL"}\NormalTok{)):}\KeywordTok{length}\NormalTok{(x))]}
\NormalTok{\})}
\KeywordTok{library}\NormalTok{(tm)}
\NormalTok{stopwords <-}\StringTok{ }\KeywordTok{c}\NormalTok{(tm::}\KeywordTok{stopwords}\NormalTok{(}\StringTok{"SMART"}\NormalTok{), }\StringTok{"dr"}\NormalTok{, }\StringTok{"mr"}\NormalTok{, }\StringTok{"ms"}\NormalTok{, }\StringTok{"mrs"}\NormalTok{)  }\CommentTok{# add titles to stoplist}
\NormalTok{good2 <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) }\KeywordTok{paste}\NormalTok{(x, }\DataTypeTok{collapse =} \StringTok{" "}\NormalTok{), }\DataTypeTok{X =} \NormalTok{good_art) %>%}\StringTok{ }
\StringTok{    }\NormalTok{stringr::}\KeywordTok{str_split}\NormalTok{(}\DataTypeTok{pattern =} \StringTok{" "}\NormalTok{) %>%}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) }\KeywordTok{gsub}\NormalTok{(}\StringTok{"'"}\NormalTok{, }
    \StringTok{""}\NormalTok{, x)) %>%}\StringTok{ }\CommentTok{# remove apostrophes}
\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) }\KeywordTok{gsub}\NormalTok{(}\StringTok{"[[:punct:]]"}\NormalTok{, }\StringTok{" "}\NormalTok{, x)) %>%}\StringTok{ }\CommentTok{# replace punctuation with space}
\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) }\KeywordTok{gsub}\NormalTok{(}\StringTok{"[[:cntrl:]]"}\NormalTok{, }\StringTok{" "}\NormalTok{, x)) %>%}\StringTok{ }\CommentTok{# replace control characters with space}
\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) }\KeywordTok{gsub}\NormalTok{(}\StringTok{"^[[:space:]]+"}\NormalTok{, }\StringTok{""}\NormalTok{, x)) %>%}\StringTok{ }\CommentTok{# remove whitespace at beginning of documents}
\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) }\KeywordTok{gsub}\NormalTok{(}\StringTok{"[[:space:]]+$"}\NormalTok{, }\StringTok{""}\NormalTok{, x)) %>%}\StringTok{ }\CommentTok{# remove whitesp}
\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) }\KeywordTok{tolower}\NormalTok{(x)) %>%}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) x[!(x ==}\StringTok{ ""}\NormalTok{)]) %>%}\StringTok{ }
\StringTok{    }\CommentTok{# remove elements that are ''}
\KeywordTok{sapply}\NormalTok{(}\DataTypeTok{FUN =} \NormalTok{function(x) x[!(x %in%}\StringTok{ }\NormalTok{stopwords)])}
\CommentTok{# remove stopwords from}
\CommentTok{# http://cpsievert.github.io/LDAvis/reviews/reviews.html compute the table}
\CommentTok{# of terms:}
\NormalTok{n_min <-}\StringTok{ }\DecValTok{3}
\NormalTok{term_table <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(good2)) %>%}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{term_table <-}\StringTok{ }\NormalTok{term_table[term_table >=}\StringTok{ }\NormalTok{n_min]}
\NormalTok{vocab <-}\StringTok{ }\KeywordTok{names}\NormalTok{(term_table)}
\NormalTok{get_terms <-}\StringTok{ }\NormalTok{function(x) \{}
    \NormalTok{index <-}\StringTok{ }\KeywordTok{match}\NormalTok{(x, vocab)}
    \NormalTok{index <-}\StringTok{ }\NormalTok{index[!}\KeywordTok{is.na}\NormalTok{(index)]}
    \KeywordTok{rbind}\NormalTok{(}\KeywordTok{as.integer}\NormalTok{(index -}\StringTok{ }\DecValTok{1}\NormalTok{), }\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{length}\NormalTok{(index))))}
\NormalTok{\}}
\NormalTok{documents <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(good2, get_terms)}
\CommentTok{# Compute some statistics related to the data set:}
\NormalTok{D <-}\StringTok{ }\KeywordTok{length}\NormalTok{(documents)  }\CommentTok{# number of documents }
\NormalTok{W <-}\StringTok{ }\KeywordTok{length}\NormalTok{(vocab)  }\CommentTok{# number of terms in the vocab}
\NormalTok{doc.length <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(documents, function(x) }\KeywordTok{sum}\NormalTok{(x[}\DecValTok{2}\NormalTok{, ]))  }\CommentTok{# number of tokens per document }
\NormalTok{N <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(doc.length)  }\CommentTok{# total number of tokens in the data }
\NormalTok{term.frequency <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(term_table)  }\CommentTok{# frequencies of terms in the corpus}
\CommentTok{# MCMC and model tuning parameters:}
\NormalTok{K <-}\StringTok{ }\DecValTok{20}
\NormalTok{G <-}\StringTok{ }\DecValTok{5000}
\NormalTok{alpha <-}\StringTok{ }\FloatTok{0.02}
\NormalTok{eta <-}\StringTok{ }\FloatTok{0.02}

\CommentTok{# Fit the model:}
\KeywordTok{library}\NormalTok{(lda)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2016} \NormalTok{-}\StringTok{ }\DecValTok{3} \NormalTok{-}\StringTok{ }\DecValTok{22}\NormalTok{)}
\NormalTok{fit1 <-}\StringTok{ }\KeywordTok{lda.collapsed.gibbs.sampler}\NormalTok{(}\DataTypeTok{documents =} \NormalTok{documents, }\DataTypeTok{K =} \NormalTok{K, }\DataTypeTok{vocab =} \NormalTok{vocab, }
    \DataTypeTok{num.iterations =} \NormalTok{G, }\DataTypeTok{alpha =} \NormalTok{alpha, }\DataTypeTok{eta =} \NormalTok{eta, }\DataTypeTok{initial =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{burnin =} \DecValTok{1000}\NormalTok{, }
    \DataTypeTok{compute.log.likelihood =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2016} \NormalTok{-}\StringTok{ }\DecValTok{3} \NormalTok{-}\StringTok{ }\DecValTok{21}\NormalTok{)}
\NormalTok{fit2 <-}\StringTok{ }\KeywordTok{lda.collapsed.gibbs.sampler}\NormalTok{(}\DataTypeTok{documents =} \NormalTok{documents, }\DataTypeTok{K =} \NormalTok{K, }\DataTypeTok{vocab =} \NormalTok{vocab, }
    \DataTypeTok{num.iterations =} \NormalTok{G, }\DataTypeTok{alpha =} \NormalTok{alpha, }\DataTypeTok{eta =} \NormalTok{eta, }\DataTypeTok{initial =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{burnin =} \DecValTok{1000}\NormalTok{, }
    \DataTypeTok{compute.log.likelihood =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2016} \NormalTok{-}\StringTok{ }\DecValTok{3} \NormalTok{-}\StringTok{ }\DecValTok{23}\NormalTok{)}
\NormalTok{fit3 <-}\StringTok{ }\KeywordTok{lda.collapsed.gibbs.sampler}\NormalTok{(}\DataTypeTok{documents =} \NormalTok{documents, }\DataTypeTok{K =} \NormalTok{K, }\DataTypeTok{vocab =} \NormalTok{vocab, }
    \DataTypeTok{num.iterations =} \NormalTok{G, }\DataTypeTok{alpha =} \NormalTok{alpha, }\DataTypeTok{eta =} \NormalTok{eta, }\DataTypeTok{initial =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{burnin =} \DecValTok{1000}\NormalTok{, }
    \DataTypeTok{compute.log.likelihood =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{library}\NormalTok{(wordcloud)}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:K) \{}
    \NormalTok{cloud.data <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(fit1$topics[i, ], }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)[}\DecValTok{1}\NormalTok{:}\DecValTok{50}\NormalTok{]}
    \KeywordTok{wordcloud}\NormalTok{(}\KeywordTok{names}\NormalTok{(cloud.data), }\DataTypeTok{freq =} \NormalTok{cloud.data, }\DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\DataTypeTok{min.freq =} \DecValTok{1}\NormalTok{, }
        \DataTypeTok{rot.per =} \DecValTok{0}\NormalTok{, }\DataTypeTok{random.order =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col =} \DecValTok{1} \NormalTok{+}\StringTok{ }\NormalTok{i%%}\DecValTok{4}\NormalTok{)}
\NormalTok{\}}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:K) \{}
    \NormalTok{cloud.data <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(fit2$topics[i, ], }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)[}\DecValTok{1}\NormalTok{:}\DecValTok{50}\NormalTok{]}
    \KeywordTok{wordcloud}\NormalTok{(}\KeywordTok{names}\NormalTok{(cloud.data), }\DataTypeTok{freq =} \NormalTok{cloud.data, }\DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\DataTypeTok{min.freq =} \DecValTok{1}\NormalTok{, }
        \DataTypeTok{rot.per =} \DecValTok{0}\NormalTok{, }\DataTypeTok{random.order =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col =} \DecValTok{1} \NormalTok{+}\StringTok{ }\NormalTok{i%%}\DecValTok{4}\NormalTok{)}
\NormalTok{\}}
\KeywordTok{library}\NormalTok{(wordcloud)}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:K) \{}
    \NormalTok{cloud.data <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(fit1$topics[i, ], }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)[}\DecValTok{1}\NormalTok{:}\DecValTok{50}\NormalTok{]}
    \KeywordTok{wordcloud}\NormalTok{(}\KeywordTok{names}\NormalTok{(cloud.data), }\DataTypeTok{freq =} \NormalTok{cloud.data, }\DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\DataTypeTok{min.freq =} \DecValTok{1}\NormalTok{, }
        \DataTypeTok{rot.per =} \DecValTok{0}\NormalTok{, }\DataTypeTok{random.order =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col =} \DecValTok{1} \NormalTok{+}\StringTok{ }\NormalTok{i%%}\DecValTok{4}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-blei2012probabilistic}{}
Blei, D. M. (2012). Probabilistic topic models. \emph{Communications of
the ACM}, \emph{55}(4), 77--84.

\hypertarget{ref-blei2014build}{}
Blei, D. M. (2014). Build, compute, critique, repeat: Data analysis with
latent variable models. \emph{Annual Review of Statistics and Its
Application}, \emph{1}, 203--232.

\hypertarget{ref-blei2006dynamic}{}
Blei, D. M., \& Lafferty, J. D. (2006). Dynamic topic models. In
\emph{Proceedings of the 23rd international conference on machine
learning} (pp. 113--120). ACM.

\hypertarget{ref-blei2007correlated}{}
Blei, D. M., \& Lafferty, J. D. (2007). A correlated topic model of
science. \emph{The Annals of Applied Statistics}, 17--35.

\hypertarget{ref-blei2003latent}{}
Blei, D. M., Ng, A. Y., \& Jordan, M. I. (2003). Latent dirichlet
allocation. \emph{The Journal of Machine Learning Research}, \emph{3},
993--1022.

\hypertarget{ref-derose2014victorian}{}
DeRose, C., Roy, C., \& Boehm, F. (2014). Victorian eyes: Literary,
statistical, and artistic perspectives on victorian novels -- and
dickens's unfinished murder mystery. \emph{Significance}, \emph{11}(2),
40--43.

\hypertarget{ref-griffiths2004hierarchical}{}
Griffiths, D., \& Tenenbaum, M. (2004). Hierarchical topic models and
the nested chinese restaurant process. \emph{Advances in Neural
Information Processing Systems}, \emph{16}, 17.

\hypertarget{ref-grimmer2010bayesian}{}
Grimmer, J. (2010). A bayesian hierarchical topic model for political
texts: Measuring expressed agendas in senate press releases.
\emph{Political Analysis}, \emph{18}(1), 1--35.

\hypertarget{ref-hall2008studying}{}
Hall, D., Jurafsky, D., \& Manning, C. D. (2008). Studying the history
of ideas using topic models. In \emph{Proceedings of the conference on
empirical methods in natural language processing} (pp. 363--371).
Association for Computational Linguistics.

\hypertarget{ref-hofmann1999probabilistic}{}
Hofmann, T. (1999). Probabilistic latent semantic indexing. In
\emph{Proceedings of the 22nd annual international aCM sIGIR conference
on research and development in information retrieval} (pp. 50--57). ACM.

\hypertarget{ref-mimno2016topic}{}
Mimno, D. (2016). Topic modeling bibliography. Retrieved from
\url{http://mimno.infosci.cornell.edu/topics.html}

\hypertarget{ref-pritchard2000inference}{}
Pritchard, J. K., Stephens, M., \& Donnelly, P. (2000). Inference of
population structure using multilocus genotype data. \emph{Genetics},
\emph{155}(2), 945--959.

\hypertarget{ref-r2015}{}
R Core Team. (2015). \emph{R: A language and environment for statistical
computing}. Vienna, Austria: R Foundation for Statistical Computing.
Retrieved from \url{https://www.R-project.org/}

\hypertarget{ref-rosen2004author}{}
Rosen-Zvi, M., Griffiths, T., Steyvers, M., \& Smyth, P. (2004). The
author-topic model for authors and documents. In \emph{Proceedings of
the 20th conference on uncertainty in artificial intelligence} (pp.
487--494). AUAI Press.

\hypertarget{ref-roy2013victorian}{}
Roy, C., DeRose, C., \& Boehm, F. (2013). Victorian eyes. Retrieved from
\url{http://victorianeyes.com}

\end{document}
